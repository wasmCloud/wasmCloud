use std::collections::HashMap;
use std::num::{NonZeroU64, NonZeroUsize};
use std::sync::{Arc, OnceLock};

use aws_sdk_s3::error::SdkError;
use aws_sdk_s3::operation::create_bucket::CreateBucketOutput;
use aws_sdk_s3::operation::head_bucket::HeadBucketError;
use aws_sdk_s3::operation::head_object::{HeadObjectError, HeadObjectOutput};
use aws_sdk_s3::operation::list_buckets::ListBucketsOutput;
use aws_sdk_s3::types::ObjectIdentifier;
use aws_sdk_s3::Client as S3Client;
use bytes::Bytes;
use tokio::sync::RwLock;
use tracing::{debug, error, instrument, warn, Instrument};

use wasmcloud_provider_sdk::core::LinkDefinition;
use wasmcloud_provider_sdk::error::{ProviderInvocationError, ProviderInvocationResult};
use wasmcloud_provider_sdk::Context;

// NOTE: many of the dependencies below are generated by provider-wit-bindgen,
// thus they will not appear in source code unless you use `cargo expand`
use crate::{
    ByteStream, Chunk, ContainerId, ContainerMetadata, ContainerObjectSelector, GetObjectRequest,
    GetObjectResponse, InvocationHandler, ListObjectsRequest, ListObjectsResponse, ObjectMetadata,
    OperationResult, PutChunkRequest, PutObjectRequest, PutObjectResponse, RemoveObjectsRequest,
    StorageConfig, Timestamp,
};

const ALIAS_PREFIX: &str = "alias_";

/// number of items to return in get_objects if max_items not specified
const DEFAULT_MAX_ITEMS: i32 = 1000;

/// maximum size of message (in bytes) that we'll return from s3 (500MB)
const DEFAULT_MAX_CHUNK_SIZE_BYTES: usize = 500 * 1024 * 1024;

#[derive(Clone)]
pub struct StorageClient {
    s3_client: S3Client,
    ld: Arc<LinkDefinition>,
    aliases: Arc<HashMap<String, String>>,
}

/// Atomic that is used to change the max chunk size bytes
static MAX_CHUNK_SIZE_BYTES: OnceLock<RwLock<usize>> = OnceLock::new();
fn get_max_chunk_size_bytes() -> &'static RwLock<usize> {
    MAX_CHUNK_SIZE_BYTES.get_or_init(|| RwLock::new(DEFAULT_MAX_CHUNK_SIZE_BYTES))
}

impl StorageClient {
    pub async fn new(config: StorageConfig, ld: LinkDefinition) -> Self {
        // Set max chunk size if provided via config (used for testing)
        {
            if let Some(v) = &config.max_chunk_size_bytes {
                let mut guard = get_max_chunk_size_bytes().write().await;
                *guard = *v;
            }
        }

        let tls_use_webpki_roots = config.tls_use_webpki_roots;
        let mut aliases = config.aliases.clone();
        let mut s3_config = aws_sdk_s3::Config::from(&config.configure_aws().await)
            .to_builder()
            // Since minio requires force path style,
            // turn it on since it's disabled by default
            // due to deprecation by AWS.
            // https://github.com/awslabs/aws-sdk-rust/issues/390
            .force_path_style(true);

        // In test configuration(s) we can use a client that does not require native roots
        // so that requests will work in a hermetic build environment
        if let Some(true) = tls_use_webpki_roots {
            use aws_smithy_runtime::client::http::hyper_014::HyperClientBuilder;
            let https_connector = hyper_rustls::HttpsConnectorBuilder::new()
                .with_webpki_roots()
                .https_or_http()
                .enable_http1()
                .enable_http2()
                .build();
            let http_client = HyperClientBuilder::new().build(https_connector);
            s3_config = s3_config.http_client(http_client);
        }
        let s3_config = s3_config.build();

        let s3_client = aws_sdk_s3::Client::from_conf(s3_config);

        for (k, v) in ld.values.iter() {
            if let Some(alias) = k.strip_prefix(ALIAS_PREFIX) {
                if alias.is_empty() || v.is_empty() {
                    error!("invalid bucket alias_ key and value must not be empty");
                } else {
                    aliases.insert(alias.to_string(), v.to_string());
                }
            }
        }
        StorageClient {
            s3_client,
            ld: Arc::new(ld),
            aliases: Arc::new(aliases),
        }
    }

    /// perform alias lookup on bucket name
    /// This can be used either for giving shortcuts to actors in the linkdefs, for example:
    /// - actor could use bucket names "alias_today", "alias_images", etc. and the linkdef aliases
    ///   will remap them to the real bucket name
    /// The 'alias_' prefix is not required, so this also works as a general redirect capability
    pub fn unalias<'n, 's: 'n>(&'s self, bucket_or_alias: &'n str) -> &'n str {
        debug!(%bucket_or_alias, aliases = ?self.aliases);
        let name = bucket_or_alias
            .strip_prefix(ALIAS_PREFIX)
            .unwrap_or(bucket_or_alias);
        if let Some(name) = self.aliases.get(name) {
            name.as_ref()
        } else {
            name
        }
    }

    // Allow overriding chunk size for testing
    fn max_chunk_size(&self) -> usize {
        if let Ok(var) = std::env::var("MAX_CHUNK_SIZE_BYTES") {
            if let Ok(size) = var.parse::<u32>() {
                return size as usize;
            }
        }
        match get_max_chunk_size_bytes().try_read() {
            Ok(v) => *v,
            Err(_) => DEFAULT_MAX_CHUNK_SIZE_BYTES,
        }
    }

    /// Perform any cleanup necessary for a link + s3 connection
    pub async fn close(&self) {
        debug!(actor_id = %self.ld.actor_id, "blobstore-s3 dropping linkdef");
        // If there were any https clients, caches, or other link-specific data,
        // we would delete those here
    }

    /// Check whether a container exists
    #[instrument(level = "debug", skip(self, ctx, arg), fields(actor_id = ?ctx.actor, bucket_id = %self.unalias(arg)))]
    pub async fn container_exists(
        &self,
        ctx: &Context,
        arg: &ContainerId,
    ) -> ProviderInvocationResult<bool> {
        let bucket_id = self.unalias(arg);
        match self.s3_client.head_bucket().bucket(bucket_id).send().await {
            Ok(_) => Ok(true),
            Err(se) => match se.into_service_error() {
                HeadBucketError::NotFound(_) => Ok(false),
                e => {
                    error!(err = ?e, "Unable to head bucket");
                    Err(ProviderInvocationError::Provider(format!(
                        "unable to head bucket: {e}"
                    )))
                }
            },
        }
    }

    /// Creates container if it does not exist
    #[instrument(level = "debug", skip(self, ctx, arg), fields(actor_id = ?ctx.actor, bucket_id = %self.unalias(arg)))]
    pub async fn create_container(
        &self,
        ctx: &Context,
        arg: &ContainerId,
    ) -> ProviderInvocationResult<()> {
        let bucket_id = self.unalias(arg);

        // If the container already exists, skip creating it
        if let Ok(true) = self.container_exists(ctx, &String::from(bucket_id)).await {
            return Ok(());
        }

        // Ensure the bucket name is valid
        if let Err(e) = validate_bucket_name(bucket_id) {
            error!("invalid bucket name");
            return Err(ProviderInvocationError::Provider(format!(
                "Invalid bucket name Bucket({bucket_id}): {e}",
            )));
        }

        // Create the bucket
        match self
            .s3_client
            .create_bucket()
            .bucket(bucket_id)
            .send()
            .await
        {
            Ok(CreateBucketOutput { location, .. }) => {
                debug!(?location, "bucket created");
                Ok(())
            }
            Err(SdkError::ServiceError(svc_err)) => {
                error!(
                    err = ?svc_err,
                    "service error",
                );
                Err(ProviderInvocationError::Provider(format!(
                    "service error: {svc_err:?}"
                )))
            }
            Err(e) => {
                error!(
                    err = %e,
                    "unexpected error",
                );
                Err(ProviderInvocationError::Provider(format!(
                    "unexpected error: {e}"
                )))
            }
        }
    }

    #[instrument(level = "debug", skip(self, ctx, arg), fields(actor_id = ?ctx.actor, bucket_id = %self.unalias(arg)))]
    pub async fn get_container_info(
        &self,
        ctx: &Context,
        arg: &ContainerId,
    ) -> ProviderInvocationResult<ContainerMetadata> {
        let bucket_id = self.unalias(arg);
        match self.s3_client.head_bucket().bucket(bucket_id).send().await {
            Ok(_) => Ok(ContainerMetadata {
                container_id: bucket_id.to_string(),
                // unfortunately, HeadBucketOut doesn't include any information
                // so we can't fill in creation date
                created_at: None,
            }),
            Err(se) => match se.into_service_error() {
                HeadBucketError::NotFound(_) => Err(ProviderInvocationError::Provider(format!(
                    "bucket [{bucket_id}] not found"
                ))),
                e => Err(ProviderInvocationError::Provider(format!(
                    "unexpected error: {e}"
                ))),
            },
        }
    }

    #[instrument(level = "debug", skip(self, ctx), fields(actor_id = ?ctx.actor))]
    pub async fn list_containers(
        &self,
        ctx: &Context,
    ) -> ProviderInvocationResult<Vec<ContainerMetadata>> {
        match self.s3_client.list_buckets().send().await {
            Ok(ListBucketsOutput {
                buckets: Some(list),
                ..
            }) => Ok(list
                .iter()
                .map(|bucket| ContainerMetadata {
                    container_id: bucket.name.clone().unwrap_or_default(),
                    created_at: bucket.creation_date.map(Timestamp::from),
                })
                .collect()),
            Ok(ListBucketsOutput { buckets: None, .. }) => Ok(Vec::new()),
            Err(SdkError::ServiceError(svc_err)) => {
                error!(err = ?svc_err, "service error");
                Err(ProviderInvocationError::Provider(format!(
                    "service error: {svc_err:?}"
                )))
            }
            Err(e) => {
                error!(err = %e, "unexpected error");
                Err(ProviderInvocationError::Provider(format!(
                    "unexpected error: {e}"
                )))
            }
        }
    }

    #[instrument(level = "debug", skip(self, ctx), fields(actor_id = ?ctx.actor))]
    pub async fn remove_containers(
        &self,
        ctx: &Context,
        arg: &Vec<String>,
    ) -> ProviderInvocationResult<Vec<OperationResult>> {
        let mut results = Vec::with_capacity(arg.len());
        for bucket in arg.iter() {
            let bucket = self.unalias(bucket);
            match self.s3_client.delete_bucket().bucket(bucket).send().await {
                Ok(_) => results.push(OperationResult {
                    key: bucket.to_string(),
                    error: None,
                    success: true,
                }),
                Err(SdkError::ServiceError(err)) => {
                    results.push(OperationResult {
                        key: bucket.to_string(),
                        error: Some(format!("{err:?}")),
                        success: false,
                    });
                }
                Err(e) => {
                    error!(err = %e, "unexpected error");
                    return Err(ProviderInvocationError::Provider(format!(
                        "unexpected error: {}",
                        e
                    )));
                }
            }
        }
        let num_errors = results.iter().filter(|r| r.error.is_some()).count();
        if num_errors > 0 {
            error!(
                "remove_containers returned {num_errors}/{} errors",
                results.len()
            );
        }
        Ok(results)
    }

    /// Find out whether object exists
    #[instrument(level = "debug", skip(self, ctx, arg), fields(actor_id = ?ctx.actor, bucket_id = %self.unalias(&arg.container_id), object_id = %arg.object_id))]
    pub async fn object_exists(
        &self,
        ctx: &Context,
        arg: &ContainerObjectSelector,
    ) -> ProviderInvocationResult<bool> {
        let bucket_id = self.unalias(&arg.container_id);
        match self
            .s3_client
            .head_object()
            .bucket(bucket_id)
            .key(&arg.object_id)
            .send()
            .await
        {
            Ok(_) => Ok(true),
            Err(se) => match se.into_service_error() {
                HeadObjectError::NotFound(_) => Ok(false),
                e => {
                    error!(
                        err = %e,
                        "unexpected error for object_exists"
                    );
                    Err(ProviderInvocationError::Provider(format!(
                        "unexpected object_exists error: {e}"
                    )))
                }
            },
        }
    }

    /// Retrieves metadata about the object
    #[instrument(level = "debug", skip(self, ctx, arg), fields(actor_id = ?ctx.actor, bucket_id = %self.unalias(&arg.container_id), object_id = %arg.object_id))]
    pub async fn get_object_info(
        &self,
        ctx: &Context,
        arg: &ContainerObjectSelector,
    ) -> Result<ObjectMetadata, ProviderInvocationError> {
        let bucket_id = self.unalias(&arg.container_id);
        match self
            .s3_client
            .head_object()
            .bucket(bucket_id)
            .key(arg.object_id.clone())
            .send()
            .await
        {
            Ok(HeadObjectOutput {
                last_modified,
                content_length,
                content_type,
                content_encoding,
                ..
            }) => Ok(ObjectMetadata {
                container_id: bucket_id.to_string(),
                object_id: arg.object_id.clone(),
                last_modified: last_modified.map(Timestamp::from),
                content_type,
                content_encoding,
                content_length: content_length.map(|v| v as u64).unwrap_or(0),
            }),
            Err(se) => match se.into_service_error() {
                HeadObjectError::NotFound(_) => Err(ProviderInvocationError::Provider(format!(
                    "object [{bucket_id}/{}] not found",
                    &arg.object_id,
                ))),
                e => Err(ProviderInvocationError::Provider(format!(
                    "get_object_metadata failed for object [{bucket_id}/{}]: {e}",
                    &arg.object_id
                ))),
            },
        }
    }

    #[instrument(level = "debug", skip(self, ctx, arg), fields(actor_id = ?ctx.actor, bucket_id = %self.unalias(&arg.container_id), max_items = arg.max_items))]
    pub async fn list_objects(
        &self,
        ctx: &Context,
        arg: &ListObjectsRequest,
    ) -> ProviderInvocationResult<ListObjectsResponse> {
        let bucket_id = self.unalias(&arg.container_id);
        debug!("asking for list_objects bucket: {}", bucket_id);
        let mut req = self.s3_client.list_objects_v2().bucket(bucket_id);
        if let Some(max_items) = arg.max_items {
            if max_items > i32::MAX as u32 {
                // edge case to avoid panic
                return Err(ProviderInvocationError::Provider(
                    "max_items too large".into(),
                ));
            }
            req = req.max_keys(max_items as i32);
        } else {
            req = req.max_keys(DEFAULT_MAX_ITEMS);
        }
        if let Some(continuation) = &arg.continuation {
            req = req.set_continuation_token(Some(continuation.clone()));
        } else if let Some(start_with) = &arg.start_with {
            req = req.set_start_after(Some(start_with.clone()));
        }
        match req.send().await {
            Ok(list) => {
                debug!(
                    "list_objects (bucket:{}) returned {} items",
                    bucket_id,
                    list.contents.as_ref().map(|l| l.len()).unwrap_or(0)
                );
                let is_last = !list.is_truncated.ok_or_else(|| {
                    ProviderInvocationError::Provider(
                        "unexpectedly missing truncation setting on list objects input".into(),
                    )
                })?;
                let objects = match list.contents {
                    Some(items) => items
                        .iter()
                        .map(|o| ObjectMetadata {
                            container_id: bucket_id.to_string(),
                            last_modified: o.last_modified.map(Timestamp::from),
                            object_id: o.key.clone().unwrap_or_default(),
                            content_length: o.size.map(|v| v as u64).unwrap_or(0),
                            content_encoding: None,
                            content_type: None,
                        })
                        .collect(),
                    None => Vec::<ObjectMetadata>::new(),
                };
                Ok(ListObjectsResponse {
                    continuation: list.next_continuation_token,
                    objects,
                    is_last,
                })
            }
            Err(e) => {
                error!(err = %e, "unable to list objects");
                Err(ProviderInvocationError::Provider(format!(
                    "unable to list objects: {e}"
                )))
            }
        }
    }

    #[instrument(level = "debug", skip(self, ctx, arg), fields(actor_id = ?ctx.actor, bucket_id = %self.unalias(&arg.container_id)))]
    pub async fn remove_objects(
        &self,
        ctx: &Context,
        arg: &RemoveObjectsRequest,
    ) -> ProviderInvocationResult<Vec<OperationResult>> {
        let bucket_id = self.unalias(&arg.container_id);

        let delete_cmd = aws_sdk_s3::types::Delete::builder()
            .set_objects(Some(
                arg.objects
                    .iter()
                    .map(|id| ObjectIdentifier::builder().key(id).build())
                    .collect::<Result<Vec<ObjectIdentifier>, aws_sdk_s3::error::BuildError>>()
                    .map_err(|e| {
                        ProviderInvocationError::Provider(format!(
                            "failed to build object set for delete: {e}"
                        ))
                    })?,
            ))
            .quiet(true)
            .build()
            .map_err(|e| {
                ProviderInvocationError::Provider(format!("failed to build delete cmd: {e}"))
            })?;

        match self
            .s3_client
            .delete_objects()
            .bucket(bucket_id)
            .delete(delete_cmd)
            .send()
            .await
        {
            Ok(output) => {
                if let Some(errors) = output.errors {
                    let mut results = Vec::with_capacity(errors.len());
                    for e in errors.iter() {
                        results.push(OperationResult {
                            key: e.key.clone().unwrap_or_default(),
                            error: e.message.clone(),
                            success: false,
                        });
                    }
                    if !results.is_empty() {
                        error!(
                            "delete_objects returned {}/{} errors",
                            results.len(),
                            arg.objects.len()
                        );
                    }
                    Ok(results)
                } else {
                    Ok(Vec::new())
                }
            }
            Err(e) => {
                error!(err = %e, "Unable to delete objects");
                Err(ProviderInvocationError::Provider(format!(
                    "unable to delete objects: {e}"
                )))
            }
        }
    }

    #[instrument(
        level = "debug",
        skip(self, ctx, arg),
        fields(actor_id = ?ctx.actor, bucket_id = %self.unalias(&arg.chunk.container_id), object_id = %arg.chunk.object_id, offset = %arg.chunk.offset, is_last = %arg.chunk.is_last)
    )]
    pub async fn put_object(
        &self,
        ctx: &Context,
        arg: &PutObjectRequest,
    ) -> ProviderInvocationResult<PutObjectResponse> {
        let bucket_id = self.unalias(&arg.chunk.container_id);
        if !arg.chunk.is_last {
            error!("put_object for multi-part upload: not implemented!");
            return Err(ProviderInvocationError::Provider(
                "multipart upload not implemented".to_string(),
            ));
        }
        if arg.chunk.offset != 0 {
            error!("put_object with initial offset non-zero: not implemented!");
            return Err(ProviderInvocationError::Provider(
                "non-zero offset not supported".to_string(),
            ));
        }
        if arg.chunk.bytes.is_empty() {
            error!("put_object with zero bytes");
            return Err(ProviderInvocationError::Provider(
                "cannot put zero-length objects".to_string(),
            ));
        }
        // TODO: make sure put_object takes an owned `PutObjectRequest` to avoid cloning the whole chunk
        let bytes = arg.chunk.bytes.to_owned();
        match self
            .s3_client
            .put_object()
            .bucket(bucket_id)
            .key(&arg.chunk.object_id)
            .body(ByteStream::from(bytes))
            .send()
            .await
        {
            Ok(_) => Ok(PutObjectResponse { stream_id: None }),
            Err(e) => {
                error!(
                    err = %e,
                    "Error putting object",
                );
                Err(ProviderInvocationError::Provider(format!(
                    "failed to put object: {e}"
                )))
            }
        }
    }

    /// Retrieve object from s3 storage.
    #[instrument(level = "debug", skip(self, ctx, arg), fields(actor_id = ?ctx.actor, bucket_id = %self.unalias(&arg.container_id), object_id = %arg.object_id, bytes_requested = tracing::field::Empty))]
    pub async fn get_object(
        &self,
        ctx: &Context,
        arg: &GetObjectRequest,
    ) -> ProviderInvocationResult<GetObjectResponse> {
        let bucket_id = self.unalias(&arg.container_id);
        let max_chunk_size = self.max_chunk_size();

        // If the object is not found, or not readable, get_object_metadata will return error.
        let meta = self
            .get_object_metadata(ctx, bucket_id, &arg.object_id)
            .await?;

        // calculate content_length requested, with error checking for range bounds
        let bytes_requested = match (arg.range_start, arg.range_end) {
            (None, Some(end)) => meta.content_length.min(end + 1),
            (Some(start), None) if start < meta.content_length => meta.content_length - start,
            (Some(start), Some(end)) if (start <= end) && start < meta.content_length => {
                meta.content_length.min(end - start + 1)
            }
            (None, None) => meta.content_length,
            _ => 0,
        };
        tracing::span::Span::current().record(
            "bytes_requested",
            &tracing::field::display(&bytes_requested),
        );

        if bytes_requested == 0 {
            return Ok(GetObjectResponse {
                content_length: 0,
                content_encoding: meta.content_encoding.clone(),
                content_type: meta.content_type.clone(),
                initial_chunk: Some(Chunk {
                    bytes: vec![],
                    container_id: bucket_id.to_string(),
                    object_id: arg.object_id.clone(),
                    is_last: true,
                    offset: 0,
                }),
                success: true,
                error: None,
            });
        }

        let get_object_req = self
            .s3_client
            .get_object()
            .bucket(bucket_id)
            .key(&arg.object_id)
            .set_range(to_range_header(arg.range_start, arg.range_end));
        match get_object_req.send().await {
            Ok(mut object_output) => {
                let len = object_output
                    .content_length
                    .map(|v| v as u64)
                    .ok_or_else(|| {
                        ProviderInvocationError::Provider(format!(
                            "failed to parse content length [{:?}]",
                            object_output.content_length
                        ))
                    })?;
                if len > bytes_requested {
                    // either the math is wrong above, or we misunderstood the api.
                    // docs say content_length is "Size of the body in bytes"
                    error!(
                        %len,
                        "requested {} bytes but more bytes were returned!",
                        bytes_requested
                    );
                }
                let mut bytes = match object_output.body.next().await {
                    Some(Ok(bytes)) => {
                        debug!(chunk_len = %bytes.len(), "initial chunk received");
                        bytes
                    }
                    None => {
                        error!("stream ended before getting first chunk from s3");
                        return Err(ProviderInvocationError::Provider(
                            "no data received from s3".into(),
                        ));
                    }
                    Some(Err(e)) => {
                        error!(err = %e, "chunk.try_next returned error");
                        return Err(ProviderInvocationError::Provider(format!(
                            "chunk.try_next returned error: {e}"
                        )));
                    }
                };
                // determine if we need to stream additional chunks
                let bytes = if (bytes.len() as u64) < bytes_requested
                    || bytes.len() > max_chunk_size
                {
                    debug!(
                        chunk_len = %bytes.len(),
                        "Beginning streaming response. Initial S3 chunk contains {} bytes out of {}",
                        bytes.len(),
                        bytes_requested,
                    );
                    let (bytes, excess) = if bytes.len() > max_chunk_size {
                        let excess = bytes.split_off(max_chunk_size);
                        (bytes, excess)
                    } else {
                        (bytes, Bytes::new())
                    };
                    // create task to deliver remaining chunks
                    let offset = arg.range_start.unwrap_or(0) + bytes.len() as u64;
                    self.stream_from_s3(
                        ctx,
                        ContainerObjectSelector {
                            container_id: bucket_id.to_string(),
                            object_id: arg.object_id.clone(),
                        },
                        excess.into(),
                        offset,
                        offset + bytes_requested,
                        object_output.body,
                    )
                    .await;
                    Vec::from(bytes)
                } else {
                    // no streaming required - everything in first chunk
                    Vec::from(bytes)
                };
                // return first chunk
                Ok(GetObjectResponse {
                    success: true,
                    initial_chunk: Some(Chunk {
                        is_last: (bytes.len() as u64) >= bytes_requested,
                        bytes,
                        container_id: bucket_id.to_string(),
                        object_id: arg.object_id.clone(),
                        offset: arg.range_start.unwrap_or(0),
                    }),
                    content_length: bytes_requested,
                    content_type: object_output.content_type.clone(),
                    content_encoding: object_output.content_encoding.clone(),
                    error: None,
                })
            }
            Err(e) => {
                error!(
                    err = %e,
                    "Error when getting object"
                );
                Err(ProviderInvocationError::Provider(format!(
                    "error when getting object: {e}"
                )))
            }
        }
    }

    pub async fn put_chunk(
        &self,
        _ctx: &Context,
        _arg: &PutChunkRequest,
    ) -> ProviderInvocationResult<()> {
        error!("put_chunk is unimplemented");
        Err(ProviderInvocationError::Provider("not implemented".into()))
    }

    /// Retrieves metadata about the object
    #[instrument(level = "debug", skip(self, _ctx), fields(actor_id = ?_ctx.actor))]
    pub async fn get_object_metadata(
        &self,
        _ctx: &Context,
        bucket_id: &str,
        object_id: &str,
    ) -> Result<ObjectMetadata, ProviderInvocationError> {
        let bucket_id = self.unalias(bucket_id);
        match self
            .s3_client
            .head_object()
            .bucket(bucket_id)
            .key(object_id)
            .send()
            .await
        {
            Ok(HeadObjectOutput {
                last_modified,
                content_length,
                content_type,
                content_encoding,
                ..
            }) => Ok(ObjectMetadata {
                container_id: bucket_id.to_string(),
                object_id: object_id.to_string(),
                last_modified: last_modified.map(Timestamp::from),
                content_type,
                content_encoding,
                content_length: content_length.map(|v| v as u64).ok_or_else(|| {
                    ProviderInvocationError::Provider(format!(
                        "failed to parse content length [{:?}]",
                        content_length
                    ))
                })?,
            }),
            Err(se) => match se.into_service_error() {
                HeadObjectError::NotFound(_) => Err(ProviderInvocationError::Provider(format!(
                    "Not found: Bucket({bucket_id}) Object({object_id})",
                ))),
                e => Err(ProviderInvocationError::Provider(format!(
                    "get_object_metadata for Bucket({bucket_id}) Object({object_id}): {e}",
                ))),
            },
        }
    }

    /// Sends bytes to actor in a single rpc message.
    /// If successful, returns number of bytes sent (same as chunk.content_length)
    #[instrument(level = "debug", skip(self, ctx, chunk), fields(actor_id = ?ctx.actor, object_id = %chunk.object_id, container_id = %self.unalias(&chunk.container_id)))]
    pub async fn send_chunk(
        &self,
        ctx: &Context,
        mut chunk: Chunk,
    ) -> Result<u64, ProviderInvocationError> {
        chunk.container_id = self.unalias(&chunk.container_id).to_string();
        let receiver = InvocationHandler::new(&self.ld);
        let object_id = chunk.object_id.clone();
        let actor_id = &self.ld.actor_id;
        let container_id = chunk.container_id.clone();
        let chunk_bytes =
            NonZeroU64::try_from(NonZeroUsize::try_from(chunk.bytes.len()).map_err(|e| {
                ProviderInvocationError::Provider(format!(
                    "failed to parse chunk length [{}]: {e}",
                    chunk.bytes.len()
                ))
            })?)
            .map_err(|e| {
                ProviderInvocationError::Provider(format!("failed to convert chunk length: {e}"))
            })?;
        if let Err(e) = receiver.receive_chunk(chunk).await {
            error!(err = %e, "sending chunk error");
            Err(ProviderInvocationError::Provider(format!(
                "sending chunk error: Bucket({container_id}) Object({object_id}) to Actor({actor_id}): {e}",
            )))
        } else {
            Ok(chunk_bytes.into())
        }
    }

    /// send any-size array of bytes to actor via streaming api,
    /// as one or more chunks of length <= MAX_CHUNK_SIZE
    /// Returns total number of bytes sent: (bytes.len())
    #[instrument(level = "debug", skip(self, ctx, cobj, bytes), fields(actor_id = ?ctx.actor, bucket_id = %self.unalias(&cobj.container_id), object_id = %cobj.object_id))]
    async fn stream_bytes(
        &self,
        ctx: &Context,
        offset: u64,
        end_range: u64, // last byte (inclusive) in requested range
        cobj: &ContainerObjectSelector,
        bytes: &[u8],
    ) -> Result<u64, ProviderInvocationError> {
        let bucket_id = self.unalias(&cobj.container_id);
        let mut bytes_sent = 0u64;
        let bytes_to_send = bytes.len() as u64;
        while bytes_sent < bytes_to_send {
            let chunk_offset = offset + bytes_sent;
            let chunk_len = (self.max_chunk_size() as u64).min(bytes_to_send - bytes_sent);
            let Some(chunk_bytes) = NonZeroU64::new(
                self.send_chunk(
                    ctx,
                    Chunk {
                        is_last: offset + chunk_len > end_range,
                        bytes: bytes[bytes_sent as usize..(bytes_sent + chunk_len) as usize]
                            .to_vec(),
                        offset: chunk_offset,
                        container_id: bucket_id.to_string(),
                        object_id: cobj.object_id.clone(),
                    },
                )
                .await?,
            ) else {
                return Err(ProviderInvocationError::Provider(
                    "sent chunk successfully but actor returned 0 bytes received.".to_string(),
                ));
            };
            bytes_sent += chunk_bytes.get();
        }
        Ok(bytes_sent)
    }

    /// Async tokio task to accept chunks from S3 and send to actor.
    /// `container_object` has the names of the container (bucket) and object to be streamed,
    /// `excess` contains optional bytes from the first s3 stream chunk that didn't fit
    ///    in the initial message
    /// `offset` is the current offset within object that we are returning to the actor
    ///    (on entry, this should be the initial range offset requested plus the number
    ///    of bytes already sent to the actor in the GetObjectResponse)
    /// `end_range` the byte offset (inclusive) of the last byte to be returned to the client
    async fn stream_from_s3(
        &self,
        ctx: &Context,
        mut container_object: ContainerObjectSelector,
        excess: Vec<u8>, // excess bytes from first chunk
        offset: u64,
        end_range: u64, // last object offset in requested range (inclusive),
        mut stream: ByteStream,
    ) {
        let ctx = ctx.clone();
        let this = self.clone();
        container_object.container_id = self.unalias(&container_object.container_id).to_string();
        let actor_id = ctx.actor.clone();
        let excess_len = excess.len();
        tokio::spawn(
            async move {
                let mut offset = offset;
                if !excess.is_empty() {
                    offset += this
                        .stream_bytes(&ctx, offset, end_range, &container_object, &excess)
                        .await?;
                    if offset > end_range {
                        return Ok::<(), ProviderInvocationError>(());
                    }
                }

                while let Some(Ok(bytes)) = stream.next().await {
                    if bytes.is_empty() {
                        warn!("object stream returned zero bytes, quitting stream");
                        break;
                    }
                    offset += this
                        .stream_bytes(&ctx, offset, end_range, &container_object, &bytes)
                        .await?;
                    if offset > end_range {
                        break;
                    }
                }
                Ok(())
            }
            .instrument(tracing::debug_span!(
                "stream_from_s3",
                ?actor_id,
                ?excess_len,
                offset,
                end_range
            )),
        );
    }
}

impl From<aws_sdk_s3::primitives::DateTime> for Timestamp {
    fn from(dt: aws_sdk_s3::primitives::DateTime) -> Timestamp {
        Timestamp {
            sec: dt.secs() as u64,
            nsec: dt.subsec_nanos(),
        }
    }
}

// /// Translate optional s3 DateTime to optional Timestamp.
// ///
// /// # Errors
// ///
// /// Invalid times return None.
// fn to_timestamp(dt: Option<aws_sdk_s3::primitives::DateTime>) -> Option<Timestamp> {
//     dt.map(|dt| TimeStamp
//     match dt {
//         Some(dt) => match Timestamp::new(dt.secs(), dt.subsec_nanos()) {
//             Ok(t) => Some(t),
//             Err(_) => None,
//         },
//         None => None,
//     }
// }

/// Enforce some of the [S3 bucket naming rules](https://docs.aws.amazon.com/AmazonS3/latest/userguide/bucketnamingrules.html)
///
/// We don't enforce all of them (assuming amazon will also return an error),
/// and we only enforce during `create_bucket`
fn validate_bucket_name(bucket: &str) -> Result<(), &'static str> {
    if !(3usize..=63).contains(&bucket.len()) {
        return Err("bucket name must be between 3(min) and 63(max) characters");
    }
    if !bucket
        .chars()
        .all(|c| c == '.' || c == '-' || c.is_ascii_lowercase() || c.is_ascii_digit())
    {
        return Err(
            "bucket names can only contain lowercase letters, numbers, dots('.') and hyphens('-')",
        );
    }
    let c = bucket.chars().next().unwrap();
    if !(c.is_ascii_lowercase() || c.is_ascii_digit()) {
        return Err("bucket names must begin with a letter or number");
    }
    let c = bucket.chars().last().unwrap();
    if !(c.is_ascii_lowercase() || c.is_ascii_digit()) {
        return Err("bucket names must end with a letter or number");
    }

    // Not all s3 validity rules are enforced here. For example, a plain IPv4 address is not allowed.
    // Rather than keep up with all the rules, we'll let the aws sdk check and return other errors.

    Ok(())
}

/// Convert optional start/end to an http range request header value
///
/// If end is before start, the range is invalid, and per spec (https://www.w3.org/Protocols/rfc2616/rfc2616-sec14.html#sec14.35),
/// the range will be ignored.
///
/// If end is specified and start is None, start value of 0 is used. (Otherwise "bytes=-x" is interpreted as the last x bytes)
fn to_range_header(start: Option<u64>, end: Option<u64>) -> Option<String> {
    match (start, end) {
        (Some(start), Some(end)) if start <= end => Some(format!("bytes={}-{}", start, end)),
        (Some(start), None) => Some(format!("bytes={}-", start)),
        (None, Some(end)) => Some(format!("bytes=0-{}", end)),
        _ => None,
    }
}

#[cfg(test)]
mod test {
    use super::*;

    #[test]
    fn range_header() {
        assert_eq!(
            to_range_header(Some(1), Some(99)),
            Some("bytes=1-99".to_string())
        );
        assert_eq!(to_range_header(Some(10), Some(5)), None);
        assert_eq!(
            to_range_header(None, Some(99)),
            Some("bytes=0-99".to_string())
        );
        assert_eq!(
            to_range_header(Some(99), None),
            Some("bytes=99-".to_string())
        );
        assert_eq!(to_range_header(None, None), None);
    }

    #[test]
    fn bucket_name() {
        assert!(validate_bucket_name("ok").is_err(), "too short");
        assert!(
            validate_bucket_name(&format!("{:65}", 'a')).is_err(),
            "too long"
        );
        assert!(validate_bucket_name("abc").is_ok());
        assert!(
            validate_bucket_name("abc.def-ghijklmnopqrstuvwxyz.1234567890").is_ok(),
            "valid chars"
        );
        assert!(validate_bucket_name("hasCAPS").is_err(), "no caps");
        assert!(
            validate_bucket_name("has_underscpre").is_err(),
            "no underscore"
        );
        assert!(
            validate_bucket_name(".not.ok").is_err(),
            "no start with dot"
        );
        assert!(validate_bucket_name("not.ok.").is_err(), "no end with dot");
    }

    #[tokio::test]
    async fn aliases() {
        let mut ld = LinkDefinition::default();
        ld.values
            .push((format!("{}foo", ALIAS_PREFIX), "bar".to_string()));
        let client = StorageClient::new(StorageConfig::default(), ld).await;

        // no alias
        assert_eq!(client.unalias("boo"), "boo");
        // alias without prefix
        assert_eq!(client.unalias("foo"), "bar");
        // alias with prefix
        assert_eq!(client.unalias(&format!("{}foo", ALIAS_PREFIX)), "bar");
        // undefined alias
        assert_eq!(client.unalias(&format!("{}baz", ALIAS_PREFIX)), "baz");
    }
}
