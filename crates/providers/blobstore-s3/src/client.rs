use std::collections::HashMap;
use std::num::{NonZeroU64, NonZeroUsize};
use std::sync::{Arc, OnceLock};

use anyhow::{bail, Result};
use aws_sdk_s3::error::SdkError;
use aws_sdk_s3::operation::create_bucket::CreateBucketOutput;
use aws_sdk_s3::operation::head_bucket::HeadBucketError;
use aws_sdk_s3::operation::head_object::{HeadObjectError, HeadObjectOutput};
use aws_sdk_s3::operation::list_buckets::ListBucketsOutput;
use aws_sdk_s3::types::ObjectIdentifier;
use aws_sdk_s3::Client as S3Client;
use bytes::Bytes;
use tokio::sync::RwLock;
use tracing::{debug, error, instrument, warn, Instrument};

use wasmcloud_provider_wit_bindgen::deps::wasmcloud_provider_sdk::core::ComponentId;
use wasmcloud_provider_wit_bindgen::deps::wasmcloud_provider_sdk::Context;

// NOTE: many of the dependencies below are generated by provider-wit-bindgen,
// thus they will not appear in source code unless you use `cargo expand`
use crate::{
    ByteStream, Chunk, ContainerId, ContainerMetadata, ContainerObjectSelector, GetObjectRequest,
    GetObjectResponse, InvocationHandler, ListObjectsRequest, ListObjectsResponse, ObjectMetadata,
    OperationResult, PutChunkRequest, PutObjectRequest, PutObjectResponse, RemoveObjectsRequest,
    StorageConfig, Timestamp,
};

const ALIAS_PREFIX: &str = "alias_";

/// number of items to return in get_objects if max_items not specified
const DEFAULT_MAX_ITEMS: i32 = 1000;

/// maximum size of message (in bytes) that we'll return from s3 (500MB)
const DEFAULT_MAX_CHUNK_SIZE_BYTES: usize = 500 * 1024 * 1024;

#[derive(Clone)]
pub struct StorageClient {
    s3_client: S3Client,
    component_id: ComponentId,
    aliases: Arc<HashMap<String, String>>,
}

/// Atomic that is used to change the max chunk size bytes
static MAX_CHUNK_SIZE_BYTES: OnceLock<RwLock<usize>> = OnceLock::new();
fn get_max_chunk_size_bytes() -> &'static RwLock<usize> {
    MAX_CHUNK_SIZE_BYTES.get_or_init(|| RwLock::new(DEFAULT_MAX_CHUNK_SIZE_BYTES))
}

impl StorageClient {
    pub async fn new(
        config: StorageConfig,
        config_values: &HashMap<String, String>,
        source_id: ComponentId,
    ) -> Self {
        // Set max chunk size if provided via config (used for testing)
        {
            if let Some(v) = &config.max_chunk_size_bytes {
                let mut guard = get_max_chunk_size_bytes().write().await;
                *guard = *v;
            }
        }

        let tls_use_webpki_roots = config.tls_use_webpki_roots;
        let mut aliases = config.aliases.clone();
        let mut s3_config = aws_sdk_s3::Config::from(&config.configure_aws().await)
            .to_builder()
            // Since minio requires force path style,
            // turn it on since it's disabled by default
            // due to deprecation by AWS.
            // https://github.com/awslabs/aws-sdk-rust/issues/390
            .force_path_style(true);

        // In test configuration(s) we can use a client that does not require native roots
        // so that requests will work in a hermetic build environment
        if let Some(true) = tls_use_webpki_roots {
            use aws_smithy_runtime::client::http::hyper_014::HyperClientBuilder;
            let https_connector = hyper_rustls::HttpsConnectorBuilder::new()
                .with_webpki_roots()
                .https_or_http()
                .enable_all_versions()
                .build();
            let http_client = HyperClientBuilder::new().build(https_connector);
            s3_config = s3_config.http_client(http_client);
        }
        let s3_config = s3_config.build();

        let s3_client = aws_sdk_s3::Client::from_conf(s3_config);

        // Process aliases
        for (k, v) in config_values.iter() {
            if let Some(alias) = k.strip_prefix(ALIAS_PREFIX) {
                if alias.is_empty() || v.is_empty() {
                    error!("invalid bucket alias_ key and value must not be empty");
                } else {
                    aliases.insert(alias.to_string(), v.to_string());
                }
            }
        }

        StorageClient {
            s3_client,
            component_id: source_id,
            aliases: Arc::new(aliases),
        }
    }

    /// perform alias lookup on bucket name
    /// This can be used either for giving shortcuts to actors in the linkdefs, for example:
    /// - actor could use bucket names "alias_today", "alias_images", etc. and the linkdef aliases
    ///   will remap them to the real bucket name
    /// The 'alias_' prefix is not required, so this also works as a general redirect capability
    pub fn unalias<'n, 's: 'n>(&'s self, bucket_or_alias: &'n str) -> &'n str {
        debug!(%bucket_or_alias, aliases = ?self.aliases);
        let name = bucket_or_alias
            .strip_prefix(ALIAS_PREFIX)
            .unwrap_or(bucket_or_alias);
        if let Some(name) = self.aliases.get(name) {
            name.as_ref()
        } else {
            name
        }
    }

    // Allow overriding chunk size for testing
    fn max_chunk_size(&self) -> usize {
        if let Ok(var) = std::env::var("MAX_CHUNK_SIZE_BYTES") {
            if let Ok(size) = var.parse::<u32>() {
                return size as usize;
            }
        }
        match get_max_chunk_size_bytes().try_read() {
            Ok(v) => *v,
            Err(_) => DEFAULT_MAX_CHUNK_SIZE_BYTES,
        }
    }

    /// Perform any cleanup necessary for a link + s3 connection
    pub async fn close(&self) {
        debug!(source_id = %self.component_id, "blobstore-s3 dropping linkdef");
        // If there were any https clients, caches, or other link-specific data,
        // we would delete those here
    }

    /// Check whether a container exists
    #[instrument(level = "debug", skip(self, ctx, arg), fields(source_id = ?ctx.actor, bucket_id = %self.unalias(arg)))]
    pub async fn container_exists(&self, ctx: &Context, arg: &ContainerId) -> bool {
        let bucket_id = self.unalias(arg);
        match self.s3_client.head_bucket().bucket(bucket_id).send().await {
            Ok(_) => true,
            Err(se) => match se.into_service_error() {
                HeadBucketError::NotFound(_) => false,
                e => {
                    error!(err = ?e, "Unable to head bucket");
                    false
                }
            },
        }
    }

    /// Creates container if it does not exist
    #[instrument(level = "debug", skip(self, ctx, arg), fields(source_id = ?ctx.actor, bucket_id = %self.unalias(arg)))]
    pub async fn create_container(&self, ctx: &Context, arg: &ContainerId) -> () {
        let bucket_id = self.unalias(arg);

        // If the container already exists, skip creating it
        if self.container_exists(ctx, &String::from(bucket_id)).await {
            return;
        }

        // Ensure the bucket name is valid
        if let Err(e) = validate_bucket_name(bucket_id) {
            error!("invalid bucket name: {e}");
            return;
        }

        // Create the bucket
        match self
            .s3_client
            .create_bucket()
            .bucket(bucket_id)
            .send()
            .await
        {
            Ok(CreateBucketOutput { location, .. }) => {
                debug!(?location, "bucket created");
            }
            Err(SdkError::ServiceError(svc_err)) => {
                error!(
                    err = ?svc_err,
                    "service error",
                );
            }
            Err(e) => {
                error!(
                    err = %e,
                    "unexpected error",
                );
            }
        }
    }

    #[instrument(level = "debug", skip(self, ctx, arg), fields(source_id = ?ctx.actor, bucket_id = %self.unalias(arg)))]
    pub async fn get_container_info(&self, ctx: &Context, arg: &ContainerId) -> ContainerMetadata {
        let bucket_id = self.unalias(arg);
        match self.s3_client.head_bucket().bucket(bucket_id).send().await {
            Ok(_) => ContainerMetadata {
                container_id: bucket_id.to_string(),
                // unfortunately, HeadBucketOut doesn't include any information
                // so we can't fill in creation date
                created_at: None,
            },
            Err(se) => match se.into_service_error() {
                HeadBucketError::NotFound(_) => {
                    error!("bucket [{bucket_id}] not found");
                    return ContainerMetadata {
                        container_id: String::default(),
                        created_at: None,
                    };
                }
                e => {
                    error!("unexpected error: {e}");
                    return ContainerMetadata {
                        container_id: String::default(),
                        created_at: None,
                    };
                }
            },
        }
    }

    #[instrument(level = "debug", skip(self, ctx), fields(source_id = ?ctx.actor))]
    pub async fn list_containers(&self, ctx: &Context) -> Vec<ContainerMetadata> {
        match self.s3_client.list_buckets().send().await {
            Ok(ListBucketsOutput {
                buckets: Some(list),
                ..
            }) => list
                .iter()
                .map(|bucket| ContainerMetadata {
                    container_id: bucket.name.clone().unwrap_or_default(),
                    created_at: bucket.creation_date.map(Timestamp::from),
                })
                .collect(),
            Ok(ListBucketsOutput { buckets: None, .. }) => Vec::new(),
            Err(SdkError::ServiceError(svc_err)) => {
                error!(err = ?svc_err, "service error");
                Vec::new()
            }
            Err(e) => {
                error!(err = %e, "unexpected error");
                Vec::new()
            }
        }
    }

    #[instrument(level = "debug", skip(self, ctx), fields(source_id = ?ctx.actor))]
    pub async fn remove_containers(
        &self,
        ctx: &Context,
        arg: &Vec<String>,
    ) -> Vec<OperationResult> {
        let mut results = Vec::with_capacity(arg.len());
        for bucket in arg.iter() {
            let bucket = self.unalias(bucket);
            match self.s3_client.delete_bucket().bucket(bucket).send().await {
                Ok(_) => results.push(OperationResult {
                    key: bucket.to_string(),
                    error: None,
                    success: true,
                }),
                Err(SdkError::ServiceError(err)) => {
                    results.push(OperationResult {
                        key: bucket.to_string(),
                        error: Some(format!("{err:?}")),
                        success: false,
                    });
                }
                Err(e) => {
                    error!(err = %e, "unexpected error");
                }
            }
        }
        let num_errors = results.iter().filter(|r| r.error.is_some()).count();
        if num_errors > 0 {
            error!(
                "remove_containers returned {num_errors}/{} errors",
                results.len()
            );
        }
        results
    }

    /// Find out whether object exists
    #[instrument(level = "debug", skip(self, ctx, arg), fields(source_id = ?ctx.actor, bucket_id = %self.unalias(&arg.container_id), object_id = %arg.object_id))]
    pub async fn object_exists(&self, ctx: &Context, arg: &ContainerObjectSelector) -> bool {
        let bucket_id = self.unalias(&arg.container_id);
        match self
            .s3_client
            .head_object()
            .bucket(bucket_id)
            .key(&arg.object_id)
            .send()
            .await
        {
            Ok(_) => true,
            Err(se) => match se.into_service_error() {
                HeadObjectError::NotFound(_) => false,
                e => {
                    error!(
                        err = %e,
                        "unexpected error for object_exists"
                    );
                    false
                }
            },
        }
    }

    /// Retrieves metadata about the object
    #[instrument(level = "debug", skip(self, ctx, arg), fields(source_id = ?ctx.actor, bucket_id = %self.unalias(&arg.container_id), object_id = %arg.object_id))]
    pub async fn get_object_info(
        &self,
        ctx: &Context,
        arg: &ContainerObjectSelector,
    ) -> ObjectMetadata {
        let bucket_id = self.unalias(&arg.container_id);
        match self
            .s3_client
            .head_object()
            .bucket(bucket_id)
            .key(arg.object_id.clone())
            .send()
            .await
        {
            Ok(HeadObjectOutput {
                last_modified,
                content_length,
                content_type,
                content_encoding,
                ..
            }) => ObjectMetadata {
                container_id: bucket_id.to_string(),
                object_id: arg.object_id.clone(),
                last_modified: last_modified.map(Timestamp::from),
                content_type,
                content_encoding,
                content_length: content_length.map(|v| v as u64).unwrap_or(0),
            },
            Err(se) => match se.into_service_error() {
                HeadObjectError::NotFound(_) => {
                    error!("object [{bucket_id}/{}] not found", arg.object_id);
                    return ObjectMetadata {
                        container_id: String::default(),
                        content_encoding: None,
                        content_length: 0,
                        content_type: None,
                        last_modified: None,
                        object_id: String::default(),
                    };
                }
                e => {
                    error!(
                        "get_object_metadata failed for object [{bucket_id}/{}]: {e}",
                        arg.object_id
                    );
                    return ObjectMetadata {
                        container_id: String::default(),
                        content_encoding: None,
                        content_length: 0,
                        content_type: None,
                        last_modified: None,
                        object_id: String::default(),
                    };
                }
            },
        }
    }

    #[instrument(level = "debug", skip(self, ctx, arg), fields(source_id = ?ctx.actor, bucket_id = %self.unalias(&arg.container_id), max_items = arg.max_items))]
    pub async fn list_objects(
        &self,
        ctx: &Context,
        arg: &ListObjectsRequest,
    ) -> ListObjectsResponse {
        let bucket_id = self.unalias(&arg.container_id);
        debug!("asking for list_objects bucket: {}", bucket_id);

        let mut req = self.s3_client.list_objects_v2().bucket(bucket_id);

        if let Some(max_items) = arg.max_items {
            if max_items > i32::MAX as u32 {
                error!("max items too large");
                return ListObjectsResponse {
                    continuation: None,
                    is_last: true,
                    objects: vec![],
                };
            }
            req = req.max_keys(max_items as i32);
        } else {
            req = req.max_keys(DEFAULT_MAX_ITEMS);
        }

        if let Some(continuation) = &arg.continuation {
            req = req.set_continuation_token(Some(continuation.clone()));
        } else if let Some(start_with) = &arg.start_with {
            req = req.set_start_after(Some(start_with.clone()));
        }

        match req.send().await {
            Ok(list) => {
                debug!(
                    "list_objects (bucket:{}) returned {} items",
                    bucket_id,
                    list.contents.as_ref().map(|l| l.len()).unwrap_or(0)
                );

                let is_last = match list.is_truncated {
                    Some(false) => true,
                    _ => {
                        return ListObjectsResponse {
                            continuation: None,
                            is_last: true,
                            objects: vec![],
                        };
                    }
                };

                let objects = match list.contents {
                    Some(items) => items
                        .iter()
                        .map(|o| ObjectMetadata {
                            container_id: bucket_id.to_string(),
                            last_modified: o.last_modified.map(Timestamp::from),
                            object_id: o.key.clone().unwrap_or_default(),
                            content_length: o.size.map(|v| v as u64).unwrap_or(0),
                            content_encoding: None,
                            content_type: None,
                        })
                        .collect(),
                    None => Vec::<ObjectMetadata>::new(),
                };
                ListObjectsResponse {
                    continuation: list.next_continuation_token,
                    objects,
                    is_last,
                }
            }
            Err(e) => {
                error!(err = %e, "unable to list objects");
                ListObjectsResponse {
                    continuation: None,
                    is_last: true,
                    objects: vec![],
                }
            }
        }
    }

    #[instrument(level = "debug", skip(self, ctx, arg), fields(source_id = ?ctx.actor, bucket_id = %self.unalias(&arg.container_id)))]
    pub async fn remove_objects(
        &self,
        ctx: &Context,
        arg: &RemoveObjectsRequest,
    ) -> Vec<OperationResult> {
        let bucket_id = self.unalias(&arg.container_id);

        let delete_cmd = match aws_sdk_s3::types::Delete::builder()
            .set_objects(Some(
                match arg
                    .objects
                    .iter()
                    .map(|id| ObjectIdentifier::builder().key(id).build())
                    .collect::<Result<Vec<ObjectIdentifier>, aws_sdk_s3::error::BuildError>>()
                {
                    Ok(v) => v,
                    Err(e) => {
                        error!("failed to build object set for delete: {e}");
                        return Vec::new();
                    }
                },
            ))
            .quiet(true)
            .build()
        {
            Ok(delete_cmd) => delete_cmd,
            Err(e) => {
                error!("failed to build delete cmd: {e}");
                return Vec::new();
            }
        };

        match self
            .s3_client
            .delete_objects()
            .bucket(bucket_id)
            .delete(delete_cmd)
            .send()
            .await
        {
            Ok(output) => {
                if let Some(errors) = output.errors {
                    let mut results = Vec::with_capacity(errors.len());
                    for e in errors.iter() {
                        results.push(OperationResult {
                            key: e.key.clone().unwrap_or_default(),
                            error: e.message.clone(),
                            success: false,
                        });
                    }
                    if !results.is_empty() {
                        error!(
                            "delete_objects returned {}/{} errors",
                            results.len(),
                            arg.objects.len()
                        );
                    }
                    results
                } else {
                    Vec::new()
                }
            }
            Err(e) => {
                error!(err = %e, "Unable to delete objects");
                Vec::new()
            }
        }
    }

    #[instrument(
        level = "debug",
        skip(self, ctx, arg),
        fields(source_id = ?ctx.actor, bucket_id = %self.unalias(&arg.chunk.container_id), object_id = %arg.chunk.object_id, offset = %arg.chunk.offset, is_last = %arg.chunk.is_last)
    )]
    pub async fn put_object(&self, ctx: &Context, arg: &PutObjectRequest) -> PutObjectResponse {
        let bucket_id = self.unalias(&arg.chunk.container_id);
        if !arg.chunk.is_last {
            error!("put_object for multi-part upload: not implemented!");
            return PutObjectResponse { stream_id: None };
        }
        if arg.chunk.offset != 0 {
            error!("put_object with initial offset non-zero: not implemented!");
            return PutObjectResponse { stream_id: None };
        }
        if arg.chunk.bytes.is_empty() {
            error!("put_object with zero bytes");
            return PutObjectResponse { stream_id: None };
        }
        // TODO: make sure put_object takes an owned `PutObjectRequest` to avoid cloning the whole chunk
        let bytes = arg.chunk.bytes.to_owned();
        match self
            .s3_client
            .put_object()
            .bucket(bucket_id)
            .key(&arg.chunk.object_id)
            .body(ByteStream::from(bytes))
            .send()
            .await
        {
            Ok(_) => PutObjectResponse { stream_id: None },
            Err(e) => {
                error!(
                    err = %e,
                    "Error putting object",
                );
                PutObjectResponse { stream_id: None }
            }
        }
    }

    /// Retrieve object from s3 storage.
    #[instrument(level = "debug", skip(self, ctx, arg), fields(source_id = ?ctx.actor, bucket_id = %self.unalias(&arg.container_id), object_id = %arg.object_id, bytes_requested = tracing::field::Empty))]
    pub async fn get_object(&self, ctx: &Context, arg: &GetObjectRequest) -> GetObjectResponse {
        let bucket_id = self.unalias(&arg.container_id);
        let max_chunk_size = self.max_chunk_size();

        // If the object is not found, or not readable, get_object_metadata will return error.
        let meta = self
            .get_object_metadata(ctx, bucket_id, &arg.object_id)
            .await;

        // calculate content_length requested, with error checking for range bounds
        let bytes_requested = match (arg.range_start, arg.range_end) {
            (None, Some(end)) => meta.content_length.min(end + 1),
            (Some(start), None) if start < meta.content_length => meta.content_length - start,
            (Some(start), Some(end)) if (start <= end) && start < meta.content_length => {
                meta.content_length.min(end - start + 1)
            }
            (None, None) => meta.content_length,
            _ => 0,
        };
        tracing::span::Span::current().record(
            "bytes_requested",
            &tracing::field::display(&bytes_requested),
        );

        if bytes_requested == 0 {
            return GetObjectResponse {
                content_length: 0,
                content_encoding: meta.content_encoding.clone(),
                content_type: meta.content_type.clone(),
                initial_chunk: Some(Chunk {
                    bytes: vec![],
                    container_id: bucket_id.to_string(),
                    object_id: arg.object_id.clone(),
                    is_last: true,
                    offset: 0,
                }),
                success: true,
                error: None,
            };
        }

        let get_object_req = self
            .s3_client
            .get_object()
            .bucket(bucket_id)
            .key(&arg.object_id)
            .set_range(to_range_header(arg.range_start, arg.range_end));
        match get_object_req.send().await {
            Ok(mut object_output) => {
                let Some(len) = object_output.content_length.map(|v| v as u64) else {
                    error!(
                        "failed to parse content length [{:?}]",
                        object_output.content_length
                    );
                    return GetObjectResponse {
                        content_encoding: None,
                        content_length: 0,
                        content_type: None,
                        error: Some("failed to resolve file subpath".into()),
                        initial_chunk: None,
                        success: false,
                    };
                };

                if len > bytes_requested {
                    // either the math is wrong above, or we misunderstood the api.
                    // docs say content_length is "Size of the body in bytes"
                    error!(
                        %len,
                        "requested {} bytes but more bytes were returned!",
                        bytes_requested
                    );
                }

                let mut bytes = match object_output.body.next().await {
                    Some(Ok(bytes)) => {
                        debug!(chunk_len = %bytes.len(), "initial chunk received");
                        bytes
                    }
                    None => {
                        error!("stream ended before getting first chunk from s3");
                        return GetObjectResponse {
                            content_encoding: None,
                            content_length: 0,
                            content_type: None,
                            error: Some("failed to resolve file subpath".into()),
                            initial_chunk: None,
                            success: false,
                        };
                    }
                    Some(Err(e)) => {
                        error!(err = %e, "chunk.try_next returned error");
                        return GetObjectResponse {
                            content_encoding: None,
                            content_length: 0,
                            content_type: None,
                            error: Some("failed to resolve file subpath".into()),
                            initial_chunk: None,
                            success: false,
                        };
                    }
                };

                // determine if we need to stream additional chunks
                let bytes = if (bytes.len() as u64) < bytes_requested
                    || bytes.len() > max_chunk_size
                {
                    debug!(
                        chunk_len = %bytes.len(),
                        "Beginning streaming response. Initial S3 chunk contains {} bytes out of {}",
                        bytes.len(),
                        bytes_requested,
                    );
                    let (bytes, excess) = if bytes.len() > max_chunk_size {
                        let excess = bytes.split_off(max_chunk_size);
                        (bytes, excess)
                    } else {
                        (bytes, Bytes::new())
                    };
                    // create task to deliver remaining chunks
                    let offset = arg.range_start.unwrap_or(0) + bytes.len() as u64;
                    self.stream_from_s3(
                        ctx,
                        ContainerObjectSelector {
                            container_id: bucket_id.to_string(),
                            object_id: arg.object_id.clone(),
                        },
                        excess.into(),
                        offset,
                        offset + bytes_requested,
                        object_output.body,
                    )
                    .await;
                    Vec::from(bytes)
                } else {
                    // no streaming required - everything in first chunk
                    Vec::from(bytes)
                };

                // return first chunk
                GetObjectResponse {
                    success: true,
                    initial_chunk: Some(Chunk {
                        is_last: (bytes.len() as u64) >= bytes_requested,
                        bytes,
                        container_id: bucket_id.to_string(),
                        object_id: arg.object_id.clone(),
                        offset: arg.range_start.unwrap_or(0),
                    }),
                    content_length: bytes_requested,
                    content_type: object_output.content_type.clone(),
                    content_encoding: object_output.content_encoding.clone(),
                    error: None,
                }
            }
            Err(e) => {
                error!(
                    err = %e,
                    "Error when getting object"
                );
                GetObjectResponse {
                    content_encoding: None,
                    content_length: 0,
                    content_type: None,
                    error: Some("failed to resolve file subpath".into()),
                    initial_chunk: None,
                    success: false,
                }
            }
        }
    }

    pub async fn put_chunk(&self, _ctx: &Context, _arg: &PutChunkRequest) {
        error!("put_chunk is unimplemented");
    }

    /// Retrieves metadata about the object
    #[instrument(level = "debug", skip(self, _ctx), fields(source_id = ?_ctx.actor))]
    pub async fn get_object_metadata(
        &self,
        _ctx: &Context,
        bucket_id: &str,
        object_id: &str,
    ) -> ObjectMetadata {
        let bucket_id = self.unalias(bucket_id);
        match self
            .s3_client
            .head_object()
            .bucket(bucket_id)
            .key(object_id)
            .send()
            .await
        {
            Ok(HeadObjectOutput {
                last_modified,
                content_length,
                content_type,
                content_encoding,
                ..
            }) => ObjectMetadata {
                container_id: bucket_id.to_string(),
                object_id: object_id.to_string(),
                last_modified: last_modified.map(Timestamp::from),
                content_type,
                content_encoding,
                content_length: match content_length.map(|v| v as u64) {
                    Some(v) => v,
                    None => {
                        error!("failed to parse content length [{:?}]", content_length);
                        return ObjectMetadata {
                            container_id: String::default(),
                            content_encoding: None,
                            content_length: 0,
                            content_type: None,
                            last_modified: None,
                            object_id: String::default(),
                        };
                    }
                },
            },
            Err(se) => match se.into_service_error() {
                HeadObjectError::NotFound(_) => {
                    error!("Not found: Bucket({bucket_id}) Object({object_id})");
                    return ObjectMetadata {
                        container_id: String::default(),
                        content_encoding: None,
                        content_length: 0,
                        content_type: None,
                        last_modified: None,
                        object_id: String::default(),
                    };
                }
                e => {
                    error!("get_object_metadata for Bucket({bucket_id}) Object({object_id}): {e}");
                    return ObjectMetadata {
                        container_id: String::default(),
                        content_encoding: None,
                        content_length: 0,
                        content_type: None,
                        last_modified: None,
                        object_id: String::default(),
                    };
                }
            },
        }
    }

    /// Sends bytes to actor in a single rpc message.
    /// If successful, returns number of bytes sent (same as chunk.content_length)
    #[instrument(level = "debug", skip(self, ctx, chunk), fields(source_id = ?ctx.actor, object_id = %chunk.object_id, container_id = %self.unalias(&chunk.container_id)))]
    pub async fn send_chunk(&self, ctx: &Context, mut chunk: Chunk) -> u64 {
        chunk.container_id = self.unalias(&chunk.container_id).to_string();
        let receiver = InvocationHandler::new(self.component_id.clone());
        let object_id = chunk.object_id.clone();
        let source_id = &self.component_id;
        let container_id = chunk.container_id.clone();

        let nonzero_usize = match NonZeroUsize::try_from(chunk.bytes.len()) {
            Ok(v) => v,
            Err(e) => {
                error!("failed to parse chunk length [{}]: {e}", chunk.bytes.len());
                return 0;
            }
        };

        let chunk_bytes = match NonZeroU64::try_from(nonzero_usize) {
            Ok(chunk_bytes) => chunk_bytes,
            Err(e) => {
                error!("failed to convert chunk length: {e}");
                return 0;
            }
        };

        if let Err(e) = receiver.receive_chunk(chunk).await {
            error!(err = %e, container_id, object_id, source_id, "sending chunk error");
            0
        } else {
            chunk_bytes.into()
        }
    }

    /// send any-size array of bytes to actor via streaming api,
    /// as one or more chunks of length <= MAX_CHUNK_SIZE
    /// Returns total number of bytes sent: (bytes.len())
    #[instrument(level = "debug", skip(self, ctx, cobj, bytes), fields(source_id = ?ctx.actor, bucket_id = %self.unalias(&cobj.container_id), object_id = %cobj.object_id))]
    async fn stream_bytes(
        &self,
        ctx: &Context,
        offset: u64,
        end_range: u64, // last byte (inclusive) in requested range
        cobj: &ContainerObjectSelector,
        bytes: &[u8],
    ) -> u64 {
        let bucket_id = self.unalias(&cobj.container_id);
        let mut bytes_sent = 0u64;
        let bytes_to_send = bytes.len() as u64;
        while bytes_sent < bytes_to_send {
            let chunk_offset = offset + bytes_sent;
            let chunk_len = (self.max_chunk_size() as u64).min(bytes_to_send - bytes_sent);

            let sent = self
                .send_chunk(
                    ctx,
                    Chunk {
                        is_last: offset + chunk_len > end_range,
                        bytes: bytes[bytes_sent as usize..(bytes_sent + chunk_len) as usize]
                            .to_vec(),
                        offset: chunk_offset,
                        container_id: bucket_id.to_string(),
                        object_id: cobj.object_id.clone(),
                    },
                )
                .await;

            let Some(chunk_bytes) = NonZeroU64::new(sent) else {
                error!("sent chunk successfully but actor returned 0 bytes received.");
                return 0;
            };
            bytes_sent += chunk_bytes.get();
        }
        bytes_sent
    }

    /// Async tokio task to accept chunks from S3 and send to actor.
    /// `container_object` has the names of the container (bucket) and object to be streamed,
    /// `excess` contains optional bytes from the first s3 stream chunk that didn't fit
    ///    in the initial message
    /// `offset` is the current offset within object that we are returning to the actor
    ///    (on entry, this should be the initial range offset requested plus the number
    ///    of bytes already sent to the actor in the GetObjectResponse)
    /// `end_range` the byte offset (inclusive) of the last byte to be returned to the client
    async fn stream_from_s3(
        &self,
        ctx: &Context,
        mut container_object: ContainerObjectSelector,
        excess: Vec<u8>, // excess bytes from first chunk
        offset: u64,
        end_range: u64, // last object offset in requested range (inclusive),
        mut stream: ByteStream,
    ) {
        let ctx = ctx.clone();
        let this = self.clone();
        container_object.container_id = self.unalias(&container_object.container_id).to_string();
        let source_id = ctx.actor.clone();
        let excess_len = excess.len();
        tokio::spawn(
            async move {
                let mut offset = offset;
                if !excess.is_empty() {
                    offset += match this
                        .stream_bytes(&ctx, offset, end_range, &container_object, &excess)
                        .await
                    {
                        // 0 indicates stream_bytes failure
                        0 => {
                            bail!("failed to stream bytes");
                        }
                        v => v,
                    };

                    if offset > end_range {
                        return Ok::<(), anyhow::Error>(());
                    }
                }

                while let Some(Ok(bytes)) = stream.next().await {
                    if bytes.is_empty() {
                        warn!("object stream returned zero bytes, quitting stream");
                        break;
                    }

                    offset += match this
                        .stream_bytes(&ctx, offset, end_range, &container_object, &bytes)
                        .await
                    {
                        // 0 indicates stream_bytes failure
                        0 => {
                            bail!("failed to stream bytes");
                        }
                        v => v,
                    };

                    if offset > end_range {
                        break;
                    }
                }
                Ok(())
            }
            .instrument(tracing::debug_span!(
                "stream_from_s3",
                ?source_id,
                ?excess_len,
                offset,
                end_range
            )),
        );
    }
}

impl From<aws_sdk_s3::primitives::DateTime> for Timestamp {
    fn from(dt: aws_sdk_s3::primitives::DateTime) -> Timestamp {
        Timestamp {
            sec: dt.secs() as u64,
            nsec: dt.subsec_nanos(),
        }
    }
}

// /// Translate optional s3 DateTime to optional Timestamp.
// ///
// /// # Errors
// ///
// /// Invalid times return None.
// fn to_timestamp(dt: Option<aws_sdk_s3::primitives::DateTime>) -> Option<Timestamp> {
//     dt.map(|dt| TimeStamp
//     match dt {
//         Some(dt) => match Timestamp::new(dt.secs(), dt.subsec_nanos()) {
//             Ok(t) => Some(t),
//             Err(_) => None,
//         },
//         None => None,
//     }
// }

/// Enforce some of the [S3 bucket naming rules](https://docs.aws.amazon.com/AmazonS3/latest/userguide/bucketnamingrules.html)
///
/// We don't enforce all of them (assuming amazon will also return an error),
/// and we only enforce during `create_bucket`
fn validate_bucket_name(bucket: &str) -> Result<(), &'static str> {
    if !(3usize..=63).contains(&bucket.len()) {
        return Err("bucket name must be between 3(min) and 63(max) characters");
    }
    if !bucket
        .chars()
        .all(|c| c == '.' || c == '-' || c.is_ascii_lowercase() || c.is_ascii_digit())
    {
        return Err(
            "bucket names can only contain lowercase letters, numbers, dots('.') and hyphens('-')",
        );
    }
    let c = bucket.chars().next().unwrap();
    if !(c.is_ascii_lowercase() || c.is_ascii_digit()) {
        return Err("bucket names must begin with a letter or number");
    }
    let c = bucket.chars().last().unwrap();
    if !(c.is_ascii_lowercase() || c.is_ascii_digit()) {
        return Err("bucket names must end with a letter or number");
    }

    // Not all s3 validity rules are enforced here. For example, a plain IPv4 address is not allowed.
    // Rather than keep up with all the rules, we'll let the aws sdk check and return other errors.

    Ok(())
}

/// Convert optional start/end to an http range request header value
///
/// If end is before start, the range is invalid, and per spec (https://www.w3.org/Protocols/rfc2616/rfc2616-sec14.html#sec14.35),
/// the range will be ignored.
///
/// If end is specified and start is None, start value of 0 is used. (Otherwise "bytes=-x" is interpreted as the last x bytes)
fn to_range_header(start: Option<u64>, end: Option<u64>) -> Option<String> {
    match (start, end) {
        (Some(start), Some(end)) if start <= end => Some(format!("bytes={}-{}", start, end)),
        (Some(start), None) => Some(format!("bytes={}-", start)),
        (None, Some(end)) => Some(format!("bytes=0-{}", end)),
        _ => None,
    }
}

#[cfg(test)]
mod test {
    use super::*;

    #[test]
    fn range_header() {
        assert_eq!(
            to_range_header(Some(1), Some(99)),
            Some("bytes=1-99".to_string())
        );
        assert_eq!(to_range_header(Some(10), Some(5)), None);
        assert_eq!(
            to_range_header(None, Some(99)),
            Some("bytes=0-99".to_string())
        );
        assert_eq!(
            to_range_header(Some(99), None),
            Some("bytes=99-".to_string())
        );
        assert_eq!(to_range_header(None, None), None);
    }

    #[test]
    fn bucket_name() {
        assert!(validate_bucket_name("ok").is_err(), "too short");
        assert!(
            validate_bucket_name(&format!("{:65}", 'a')).is_err(),
            "too long"
        );
        assert!(validate_bucket_name("abc").is_ok());
        assert!(
            validate_bucket_name("abc.def-ghijklmnopqrstuvwxyz.1234567890").is_ok(),
            "valid chars"
        );
        assert!(validate_bucket_name("hasCAPS").is_err(), "no caps");
        assert!(
            validate_bucket_name("has_underscpre").is_err(),
            "no underscore"
        );
        assert!(
            validate_bucket_name(".not.ok").is_err(),
            "no start with dot"
        );
        assert!(validate_bucket_name("not.ok.").is_err(), "no end with dot");
    }

    #[tokio::test]
    async fn aliases() {
        let client = StorageClient::new(
            StorageConfig::default(),
            &HashMap::from([(format!("{ALIAS_PREFIX}foo"), "bar".into())]),
            "some-source".to_string(),
        )
        .await;

        // no alias
        assert_eq!(client.unalias("boo"), "boo");
        // alias without prefix
        assert_eq!(client.unalias("foo"), "bar");
        // alias with prefix
        assert_eq!(client.unalias(&format!("{}foo", ALIAS_PREFIX)), "bar");
        // undefined alias
        assert_eq!(client.unalias(&format!("{}baz", ALIAS_PREFIX)), "baz");
    }
}
